{
    "training_params": {
        "batch_size": 32,
        "epochs": 6,
        "lr": 0.0001
    },
    "architecture_params": {
        "sizes": [3, 32, 64, 128, 256, 512, 784],
        "h": 218,
        "w": 178,
        "num_dense_layers": 2,
        "z_dim": 128
    },
    "dataset_params": {
        "name": "celeba",
        "hyperparams": {
            "batch_size" : 32,
            "split": 0.80
        }
    }
}
